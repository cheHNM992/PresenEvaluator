# ãƒ—ãƒ¬ã‚¼ãƒ³è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

import streamlit as st
import os
import sys
import re
import pptx
import openai
import numpy as np
import base64
import pandas as pd
import shutil
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE
from datetime import datetime

# ==== ãƒšãƒ¼ã‚¸è¨­å®š ====
st.set_page_config(page_title="AIãƒ—ãƒ¬ã‚¼ãƒ³è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# ==== ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š ====
st.markdown("""
    <style>
    .main {
        background-color: #f8f9fa;
    }
    .stMetric {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    </style>
    """, unsafe_allow_html=True)

# ==== ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜ ====
st.title("ğŸ¤ AIãƒ—ãƒ¬ã‚¼ãƒ³è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¹ãƒ©ã‚¤ãƒ‰è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€AIãŒã‚ãªãŸã®ãƒ—ãƒ¬ã‚¼ãƒ³ã‚’å¤šè§’çš„ã«åˆ†æãƒ»æ¡ç‚¹ã—ã¾ã™ã€‚")

# ==== ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ====
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    api_key = st.text_input("OpenAI API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    
    # å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’åæ˜ ï¼ˆ2026å¹´æ™‚ç‚¹ã®æœ€æ–°ã‚’æƒ³å®šï¼‰
    model_llm = st.selectbox("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« (LLM)", ["gpt-5.2-2025-12-11", "gpt-5-nano"], index=0)
    model_whisper = "whisper-1"
    
    st.info("""
    **åˆ†æé …ç›®:**
    1. å†…å®¹ (30%)
    2. ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“ (30%)
    3. è¦–è¦šè³‡æ–™ (20%)
    4. æ§‹æˆ (20%)
    """)

# APIã‚­ãƒ¼ã®ã‚»ãƒƒãƒˆ
if api_key:
    client = openai.OpenAI(api_key=api_key)
else:
    st.warning("âš ï¸ ç¶šè¡Œã™ã‚‹ã«ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ==== éŸ³å£°åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ====
def transcribe_audio(file_path):
    audio_file = open(file_path, "rb")
    response = openai.audio.transcriptions.create(
        model=model_whisper,
        file=audio_file,
        response_format="verbose_json",
        language="ja"
    )

    text = response.text
    segments = response.segments

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"transcription_{timestamp}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(text)

    return text, segments


def analyze_speech(segments):
    total_words = sum(len(seg.text.split()) for seg in segments)
    duration_minutes = (segments[-1].end - segments[0].start) / 60.0
    wpm = total_words / duration_minutes if duration_minutes else 0

    filler_words = ['ãˆãƒ¼ã¨', 'ã‚ã®', 'ãˆã£ã¨', 'ãã®']
    filler_count = sum(sum(word in seg.text for word in filler_words) for seg in segments)

    pause_lengths = [segments[i + 1].start - segments[i].end for i in range(len(segments) - 1)]
    long_pauses = sum(1 for p in pause_lengths if p > 1.0)

    return {
        "wpm": round(wpm, 2),
        "filler_count": filler_count,
        "long_pauses": long_pauses
    }


# ==== è³‡æ–™æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ====
def extract_ppt_text(file_path):
    prs = Presentation(file_path)
    slides_text = []
    for i, slide in enumerate(prs.slides):
        slide_text = "\n".join([shape.text for shape in slide.shapes if hasattr(shape, "text")])
        slides_text.append(f"ã‚¹ãƒ©ã‚¤ãƒ‰ {i + 1}:\n{slide_text}\n")
    return "\n".join(slides_text)


def extract_images_from_ppt(ppt_path, output_dir):
    prs = Presentation(ppt_path)
    image_files = []

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for i, slide in enumerate(prs.slides):
        for j, shape in enumerate(slide.shapes):
            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                image = shape.image
                image_bytes = image.blob
                image_filename = os.path.join(output_dir, f"slide_{i + 1}_image_{j + 1}.png")
                with open(image_filename, 'wb') as f:
                    f.write(image_bytes)
                image_files.append(image_filename)

    return image_files


def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


def analyze_image(image_path):
    st.write("ãƒ•ã‚¡ã‚¤ãƒ«å: {image_path}")
    base64_image = encode_image_to_base64(image_path)
    response = client.chat.completions.create(
        model=model_llm,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯ç”»åƒè§£æã®å°‚é–€å®¶ã§ã™ã€‚"},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ã“ã®ç”»åƒã«ä½•ãŒå†™ã£ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã€ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ã¨ã—ã¦é©åˆ‡ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚è¦–è¦šè³‡æ–™ã¨ã—ã¦ã®è³ªã‚‚100ç‚¹æº€ç‚¹ï¼ˆæ•´æ•°ï¼‰ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: è¦–è¦šè³‡æ–™: â—‹ç‚¹"},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                ]
            }
        ]
    )
    return response.choices[0].message.content


def extract_visual_score(image_analysis):
    pattern = r"è¦–è¦šè³‡æ–™: ([0-9]{1,3})ç‚¹"
    matches = re.findall(pattern, image_analysis)
    if matches:
        scores = [int(score) for score in matches]
        return int(sum(scores) / len(scores))
    return 0


def analyze_all_images(image_files):
    all_analyses = []
    for image_path in image_files:
        analysis = analyze_image(image_path)
        all_analyses.append(f"{image_path}:\n{analysis}\n")
    return "\n".join(all_analyses)


def analyze_slide_text(slide_text):
    response = client.chat.completions.create(
        model=model_llm,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™è©•ä¾¡è€…ã§ã™ã€‚"},
            {"role": "user", "content": f"""
ä»¥ä¸‹ã¯ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ©ã‚¤ãƒ‰å…¨æ–‡ã§ã™ã€‚

[ã‚¹ãƒ©ã‚¤ãƒ‰å…¨æ–‡]
{slide_text}

ã“ã®è³‡æ–™ã®ã‚¹ãƒ©ã‚¤ãƒ‰æ•°ã€å„ã‚¹ãƒ©ã‚¤ãƒ‰ã®æ–‡å­—é‡ã®é©åˆ‡ã•ã€å†…å®¹ã‚’è©•ä¾¡ã—ã€å…¨ä½“çš„ãªè³‡æ–™ã®è³ªã‚’ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§100ç‚¹æº€ç‚¹ï¼ˆæ•´æ•°ï¼‰ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
ãŸã ã—ã€å›³è¡¨ã‚„ç”»åƒã¯è©•ä¾¡ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚

è³‡æ–™: â—‹ç‚¹

ãã®å¾Œã«è³‡æ–™ã®è‰¯ã„ç‚¹ã¨æ”¹å–„ç‚¹ã‚’ç°¡å˜ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
"""}
        ]
    )
    return response.choices[0].message.content


def generate_evaluation_with_images(transcription, slide_text_analysis, image_analysis):
    response = client.chat.completions.create(
        model=model_llm,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ—ãƒ¬ã‚¼ãƒ³è©•ä¾¡è€…ã§ã™ã€‚"},
            {"role": "user", "content": f"""
ä»¥ä¸‹ã¯ãƒ—ãƒ¬ã‚¼ãƒ³ã®æ–‡å­—èµ·ã“ã—ã¨ã‚¹ãƒ©ã‚¤ãƒ‰è³‡æ–™ã®åˆ†æçµæœã€ãŠã‚ˆã³ç”»åƒåˆ†æçµæœã§ã™ã€‚

[æ–‡å­—èµ·ã“ã—]:
{transcription}

[ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ]:
{slide_text_analysis}

[ç”»åƒåˆ†æ]:
{image_analysis}

ä»¥ä¸‹ã®4ã¤ã®è¦³ç‚¹ï¼ˆå†…å®¹ã€ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“ã€è¦–è¦šè³‡æ–™ã€æ§‹æˆï¼‰ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œ100ç‚¹æº€ç‚¹ï¼ˆæ•´æ•°ï¼‰ã§è©•ä¾¡ã—ã€ç°¡å˜ãªç†ç”±ã¨æ”¹å–„ç‚¹ã€é•·æ‰€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
æœ€å¾Œã«3ã¤ã®æ”¹å–„ç‚¹ã¨å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚ç¤ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯å¿…ãšä»¥ä¸‹ã¨ã—ã¦ãã ã•ã„ï¼š
å†…å®¹: â—‹ç‚¹
ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“: â—‹ç‚¹
è¦–è¦šè³‡æ–™: â—‹ç‚¹
æ§‹æˆ: â—‹ç‚¹

ãã®å¾Œã«è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
"""}
        ]
    )
    return response.choices[0].message.content


# ==== ã‚¹ã‚³ã‚¢æŠ½å‡º ====
def extract_scores(evaluation_text):
    pattern = r"å†…å®¹: ([0-9]{1,3})ç‚¹.*?ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“: ([0-9]{1,3})ç‚¹.*?è¦–è¦šè³‡æ–™: ([0-9]{1,3})ç‚¹.*?æ§‹æˆ: ([0-9]{1,3})ç‚¹"
    match = re.search(pattern, evaluation_text, re.DOTALL)

    if match:
        return {
            "å†…å®¹": int(match.group(1)),
            "ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“": int(match.group(2)),
            "è¦–è¦šè³‡æ–™": int(match.group(3)),
            "æ§‹æˆ": int(match.group(4))
        }
    return {"å†…å®¹": 0, "ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“": 0, "è¦–è¦šè³‡æ–™": 0, "æ§‹æˆ": 0}


def compute_score(sub_scores):
    weights = {"å†…å®¹": 0.3, "ãƒ—ãƒ¬ã‚¼ãƒ³æŠ€è¡“": 0.3, "è¦–è¦šè³‡æ–™": 0.2, "æ§‹æˆ": 0.2}
    total = sum(float(sub_scores[k]) * weights[k] for k in weights)
    return int(round(total, 0))

# ==== Web UI ãƒ¡ã‚¤ãƒ³å‡¦ç† ====
col1, col2 = st.columns(2)
with col1:
    audio_upload = st.file_uploader("1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['mp3', 'wav', 'm4a', 'mp4'])
with col2:
    ppt_upload = st.file_uploader("2. PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['pptx'])

if st.button("ğŸ“Š ãƒ—ãƒ¬ã‚¼ãƒ³ã‚’åˆ†æã™ã‚‹", use_container_width=True):
    if not api_key:
        st.error("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()

    os.environ["OPENAI_API_KEY"] = api_key 
    client = openai.OpenAI(api_key=api_key)

    if audio_upload and ppt_upload:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir = "temp_process"
        if not os.path.exists(temp_dir): os.makedirs(temp_dir)
        
        audio_path = os.path.join(temp_dir, audio_upload.name)
        ppt_path = os.path.join(temp_dir, ppt_upload.name)
        
        with open(audio_path, "wb") as f: f.write(audio_upload.getbuffer())
        with open(ppt_path, "wb") as f: f.write(ppt_upload.getbuffer())

        try:
            with st.status("åˆ†æä¸­...", expanded=True) as status:
                st.write("ğŸ™ï¸ éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ãƒ»åˆ†æä¸­...")
                text, segments = transcribe_audio(audio_path)
                speech_analysis = analyze_speech(segments)

                st.write("ğŸ“„ ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­...")
                slides_text = extract_ppt_text(ppt_path)
                slide_text_analysis = analyze_slide_text(slides_text)

                st.write("ğŸ–¼ï¸ ç”»åƒã‚’è§£æä¸­...")
                img_extract_dir = os.path.join(temp_dir, "extracted_images")
                image_files = extract_images_from_ppt(ppt_path, img_extract_dir)
                
                if image_files:
                    image_analysis = analyze_all_images(image_files)
                    image_visual_score = extract_visual_score(image_analysis)
                else:
                    image_analysis = "ç”»åƒã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                    image_visual_score = 0

                st.write("ğŸ¤– ç·åˆè©•ä¾¡ã‚’ç”Ÿæˆä¸­...")
                evaluation = generate_evaluation_with_images(text, slide_text_analysis, image_analysis)
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                sub_scores = extract_scores(evaluation)
                if image_visual_score == 0:
                    image_visual_score = sub_scores["è¦–è¦šè³‡æ–™"]
                sub_scores["è¦–è¦šè³‡æ–™"] = int(round((sub_scores["è¦–è¦šè³‡æ–™"] + image_visual_score) / 2, 0))
                total_score = compute_score(sub_scores)

                status.update(label="âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼", state="complete", expanded=False)

            # ==== çµæœä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ====
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_filename = f"evaluation_result_{timestamp}.txt"

            with open(result_filename, "w", encoding="utf-8") as f:
                f.write(f"==== ç·åˆå¾—ç‚¹: {total_score}ç‚¹ ====\n\n")
                f.write("==== ç·åˆè©•ä¾¡ ====\n")
                f.write(evaluation + "\n\n")
                f.write("==== éŸ³å£°åˆ†æ ====\n")
                f.write(str(speech_analysis) + "\n\n")
#                f.write("==== ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ ====\n")    #ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®åˆ†æçµæœã¨ãªã‚‰ãªã„ãŸã‚éè¡¨ç¤º
#                f.write(slide_text_analysis + "\n\n")
#                f.write("==== ç”»åƒåˆ†æ ====\n")               #å†—é•·ãªçµæœã—ã‹å‡ºåŠ›ã§ããªã„ãŸã‚éè¡¨ç¤º
#                f.write(image_analysis + "\n\n")

            print(f"\nè©•ä¾¡çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {result_filename}")

            # ==== çµæœè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³ ====
            st.divider()
            
            # ç·åˆå¾—ç‚¹ã®è¡¨ç¤º
            c1, c2 = st.columns([1, 2])
            with c1:
                st.metric(label="ç·åˆå¾—ç‚¹", value=f"{total_score} ç‚¹")
                # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                score_data = pd.DataFrame({
                    "é …ç›®": list(sub_scores.keys()),
                    "å¾—ç‚¹": list(sub_scores.values())
                })
                st.bar_chart(score_data.set_index("é …ç›®"))

            with c2:
                st.subheader("ğŸ”Š éŸ³å£°åˆ†æ")
                sc1, sc2, sc3 = st.columns(3)
                sc1.metric("è©±ã™é€Ÿã• (WPM)", speech_analysis['wpm'])
                sc2.metric("ãƒ•ã‚£ãƒ©ãƒ¼æ•°", speech_analysis['filler_count'])
                sc3.metric("é•·ã„æ²ˆé»™", speech_analysis['long_pauses'])

            st.divider()
            
            # ã‚¿ãƒ–ã«ã‚ˆã‚‹è©³ç´°è¡¨ç¤º
            tab1, tab2, tab3 = st.tabs(["ğŸ“ ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ“– æ–‡å­—èµ·ã“ã—å…¨æ–‡", "ğŸ–¼ï¸ ã‚¹ãƒ©ã‚¤ãƒ‰åˆ†æè©³ç´°"])
            
            with tab1:
                st.markdown(evaluation)
                
            with tab2:
                st.text_area("æ–‡å­—èµ·ã“ã—å†…å®¹", text, height=300)
                
            with tab3:
                st.markdown("### ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡")
                st.write(slide_text_analysis)
                if image_files:
                    st.markdown("### æŠ½å‡ºã•ã‚ŒãŸç”»åƒã¨AIã‚³ãƒ¡ãƒ³ãƒˆ")
                    for img in image_files:
                        st.image(img, width=300)

        except Exception as e:
            st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)

    else:
        st.info("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¡æ–¹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€åˆ†æãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# ==== ãƒ•ãƒƒã‚¿ãƒ¼ ====
st.markdown("---")
st.caption(f"Presentation Evaluator Pro v2.0 | Powered by {model_llm}")
